---
title: "Data processing for rough R: Hungarian roughness norms A3"
output:
  html_document: default
---

This script takes the raw data files from our Hungarian norming experiment and creates (1) a deaggregated data set ready for analysis and (2) a by-stimulus aggregated data set

Loading packages & data.

```{r, cache=T}
library(tidyverse)
library(irr)

# import responses
hun <- read_csv("../raw_data/a3_hungarian_norms/hun_roughness_ratings.csv")
# import metadata
hun_info <- read_csv("../raw_data/a3_hungarian_norms/hun_norm_stim_info.csv")
```

Since hun is a data frame as output directly by Qualtrics, it requires a substantial amount of wrangling to be usable. We first fix the column names and then extract the stimulus names, which are in the first row of the data frame. The first three rows of the data frame all contain rubbish generated by qualtrics (actually, row three is a test row), so all of these will need to be removed, but the stimulus names from the first row have to be saved as we'll need them later (to link the ratings to the stimuli). 

```{r}
# remove '_2' from timing column names:
colnames(hun) <- str_replace(colnames(hun), '2_', '')
# fix column names for roughness ratings (some of them have odd numbers...)
colnames(hun) <- str_replace(colnames(hun),
	'[.]1[_]*[1-3]*', '.roughness')

# save item names for later
item_names <- hun[1,] %>%
  dplyr::select(matches("roughness")) %>%   # selecting stimulus names specifically
  unlist () %>%                             # turn row of data frame into vector vector
  as.character()                            # character vector
```

Some further formatting before more substantial data wrangling.

```{r}
# chuck rubbish at top of file:
hun <- hun[3:nrow(hun), ] %>%
  filter(Progress == '100') %>%      # chuck incomplete responses:
  dplyr::select(-Status,             # chuck redundant columns
    -Finished, -Progress,                     
	  -DistributionChannel, -Ex1_1,
	  -Q181, -Q183_1, -Q184_1) %>%
  rename(                            # rename overall duration column:
    Overall.Dur = 
    "Duration (in seconds)")

# make all column names lower case:
colnames(hun) <- tolower(colnames(hun))
```

Some data formatting will be necessary. Here's what we need to do:

- convert data to long format
- convert factor to numeric for data columns
- convert "" to 0 for ratings
- merge with info about items
- add indicators for phonemes

We start with the long format. The indicators for phonemes will be first added to the stimulus info file and then merged with the long format data file at the end.

```{r, cache=T}
# move question no to end (for reshape)
colnames(hun) <- str_replace(colnames(hun), 'q([0-9]*)[.](.*)', '\\2.\\1')

# only interested in roughness norms, which also need to be numeric
rough_only <- dplyr::select(hun, matches('roughness')) %>%
	mutate_all(function(x) as.numeric(as.character(x)))

# NA's are true 0's:
rough_only[is.na(rough_only)] <- 0

# bind together participant ids, how long they took to complete the experiment
#  and the roughness ratings
hun_flat <- bind_cols(dplyr::select(hun, responseid, overall.dur),
                      rough_only)

# move to long format
hun_long <- gather(hun_flat, key='id', value='roughness',
	-responseid, -overall.dur) %>% 
	mutate(item = str_extract(id, '[0-9]+'),
		     item = as.integer(item))

# add orthographic stimulus descriptions:
hun_long$stimulus <- item_names[hun_long$item]

# responseid = participant
hun_long <- hun_long %>%
  rename(participant="responseid") %>%
  dplyr::select(-id,-item)
```

Quick check on inter-rater reliability.

```{r}
# add rater ids to data frame to track response ids:
rough_matrix <- as.matrix(rough_only)
row.names(rough_matrix) <- hun$responseid

# for rough only, we can check inter-rater reliability:
rough_raters <- t(rough_matrix)
print(hun_icc <- icc(rough_raters, model = 'twoway', type = 'agreement'))

# check correlations:
rough_cors <- cor(rough_raters)
range(rough_cors[rough_cors!= 1])
mean(rough_cors[rough_cors != 1])

# subject 19 is vastly different from the rest; everything is anti-correlated
# seems to have misunderstood the rating scale:
print(hung_icc_excluded <- icc(rough_raters[, -19],
	model = 'twoway', type = 'agreement'))
rough_cors <- cor(rough_raters)[-19,-19]
range(rough_cors[rough_cors!= 1])
mean(rough_cors[rough_cors != 1])

## Exclude subject 19 from main data frame:

hun_long <- hun_long %>%
  filter(participant != colnames(rough_raters)[19])
```

We now create an aggregated version of this data set (by stimuli).

```{r}
hun_aggr <- hun_long %>%
  group_by(stimulus) %>%
  summarise(
    roughness=mean(roughness)) %>%
  ungroup()
```

We now add phoneme indicators to the stimulus info file, which will then be merged with the aggregated ratings. We first locate (1) onsets, (2) onsets in stressed syllables, (3) rhymes and (4) rhymes in stressed syllables. We add each segment type in each position as a predictor to our data set (each of which codes "does position X have segment Y?" as a binary predictor), as well as a position-free version of the segments. We start by creating a list of unique segments / consonants / vowels.

```{r}
# find unique segments
unique_segments <- unique(unlist(str_split(hun_info$HunSuggestedTrs, pattern="")))
# remove syllable boundaries
unique_segments <- unique_segments[unique_segments != "."]
# arrange segments
unique_segments <- sort(unique_segments)
# consonants vs. vowels
consonants <- c("b", "r", "z", "d", "l", "t", "S", "j", "h", "k", "H", "c", "J", "p", "s", "n", "m", "v", "D", "f", "g", "T", "Z")
vowels <- unique_segments[!(unique_segments %in% consonants)]
# regex for consonants / vowels
consonants_regex <- paste0("[", paste0(consonants, collapse=""), "]")
vowels_regex <- paste0("[", paste0(vowels, collapse=""), "]")
```

We now locate these segments in different syllabic positions.

```{r}
# all onset segments
hun_info$onset <- hun_info$HunSuggestedTrs %>%
  str_extract_all(paste0('(^', consonants_regex, '+|[.]', consonants_regex, "+)")) %>%
  lapply(paste0, collapse="") %>%
  unlist()

# stressed onset segments (= word initial in Hungarian)
hun_info$stressed_onset <- hun_info$HunSuggestedTrs %>% 
  str_extract_all(paste0('^', consonants_regex, '+')) %>%
  lapply(paste0, collapse="") %>%
  unlist()

# all rhyme segments
hun_info$rhyme <- hun_info$HunSuggestedTrs %>% 
  str_extract_all(paste0('(', vowels_regex, '*', consonants_regex, '*[.]|', vowels_regex, '*', consonants_regex, "*$)")) %>%
  lapply(paste0, collapse="") %>%
  unlist()

# stressed rhyme segments (= in first syllable)
hun_info$stressed_rhyme <- str_match(hun_info$HunSuggestedTrs, 
  paste0('^', consonants_regex, '*(', vowels_regex, '*', consonants_regex, '*)'))[,2]
```

Setting up indicator variables for each segment for each type of position.

```{r}
# function for checking for segments in specific positions
# argument 1: vector of strings with all segments in relevant positions
# argument 2: list of segments to check for

find_segments <- function (strings, segments) {
  out_matrix <- matrix(0, nrow=length(strings), ncol=length(segments))
  colnames(out_matrix) <- segments
  for (s in 1:length(segments)) {
    segment <- segments[s]
    out_matrix[,s] <- str_detect(strings, segment)
  }
  return(as_tibble(out_matrix))
}

# segments in different positions
# whole word
whole_word_segs <- find_segments(hun_info$HunSuggestedTrs, unique_segments)
colnames(whole_word_segs) <- paste0("all.", colnames(whole_word_segs))
hun_info <- bind_cols(hun_info, whole_word_segs)
# onset
onset_segs <- find_segments(hun_info$onset, consonants)
colnames(onset_segs) <- paste0("onset.", colnames(onset_segs))
hun_info <- bind_cols(hun_info, onset_segs)
# stressed onset
stressed_onset_segs <- find_segments(hun_info$stressed_onset, consonants)
colnames(stressed_onset_segs) <- paste0("stressed_onset.", colnames(stressed_onset_segs))
hun_info <- bind_cols(hun_info, stressed_onset_segs)
# rhyme
rhyme_segs <- find_segments(hun_info$rhyme, unique_segments)
colnames(rhyme_segs) <- paste0("rhyme.", colnames(rhyme_segs))
hun_info <- bind_cols(hun_info, rhyme_segs)
# stressed rhyme
stressed_rhyme_segs <- find_segments(hun_info$stressed_rhyme, unique_segments)
colnames(stressed_rhyme_segs) <- paste0("stressed_rhyme.", colnames(stressed_rhyme_segs))
hun_info <- bind_cols(hun_info, stressed_rhyme_segs)
```

Now (1) we merge this with the aggregated data and (2) add presence of r/l (in the entire word) as predictors to the deaggregated data.

```{r}
# aggregated data
hun_aggr <- inner_join(hun_aggr, hun_info, by="stimulus")
write_csv(hun_aggr, "../final_data/hun_norms_aggr.csv")

# deaggregated data
hun_long <- hun_long %>%
  left_join(dplyr::select(hun_info, stimulus, etymology.source, all.r, all.l),
            by="stimulus")
write_csv(hun_long, "../final_data/hun_norms_full.csv")
```


